# SEP-Nets
This repository contains instroduction, instruction, models, prototxt and logs file along experiments for SEP-Nets introduced in the paper  ["SEP-Nets: Small and Effective Pattern Networks"](https://arxiv.org/pdf/1706.03912.pdf) by Zhe Li, Xiaoyu Wang, Xutao Lv, Tianbao Yang.

### Citting SEP-Nets
If you are using SEP-Nets, please cite as folowing:

        @article{li2017sep,
          title={SEP-Nets: Small and Effective Pattern Networks},
          author={Li, Zhe and Wang, Xiaoyu and Lv, Xutao and Yang, Tianbao},
          journal={arXiv preprint arXiv:1706.03912},
          year={2017}
        }

## Contents:
1. [Introduction](#Introduction)
2. [Instruction](#Instruction)
3. [Experiments on CIFAR10 with ResNet](#Experiments-on-CIFAR10-with-ResNet)
4. [Experiments on ImageNet with GoogleNet](#Experiments-on-ImageNet-with-GoogleNet)
5. [Experiments on ImageNet with Customized-InceptionNet](#Experiments-on-ImageNet-with-Customized-InceptionNet)
6. [Summary](#Summary)

## Introduction
SEP-Nets are very small yet powerful deep neural network structures based on the invented techniques pattern binarization, pattern residual blocks and the existing group convolution. The two invented techniques pattern binarization, pattern residual blocks are of their interests. For pattern binarization, the striking difference from most previous work on parameter binarization/quantization lies at different treatments of 1 x 1 convolutions and  k x k convolutions (k>1), where we only binarize k x k convolutions into binary patterns.  The resulting networks are referred to as pattern networks. By doing this, we show that previous deep CNNs such as GoogLeNet and Inception-type Nets can be compressed dramatically with marginal drop in performance. In light of the different functionalities of  1 x 1 (data projection/transformation) and k x k convolutions (pattern extraction), we propose a new block structure codenamed the pattern residual block that adds transformed feature maps generated by 1 x 1 convolutions to the pattern feature maps generated by k x k convolutions. Based on the above techniques, the designed SEP-Net could achieve better performance on ImageNet than using similar sized networks including recently released Google MobileNets.

## Instruction
For Justifying pattern binarization could dramtically compressed original model with marginal drop in performance, we conducted experiments in three steps (shown in the following Figure 1):
<img src= "https://user-images.githubusercontent.com/13735345/31589251-38a76466-b1c4-11e7-89b9-42def3bc47b1.png" width ="240">

Figure 1: Procedure to apply pattern binarization technique

The general instructions are the following steps: (In the folllowing three sections, we will give the detailed instructions)
1. train original model using tain_val.prototxt and solver.txt, after training process we obtain the original model;
```
caffe/build/tools/caffe train --solver solver.txt 2>&1 | tee train.log
```
2. quantilize k x k filters (k > 1, for example often k = 3, 5, or at same time quantilize both 3 and 5) in the original model, we obtained the quantilized model. Note that we first fine-tune offset value to quantilize to from range(0.02, 0.09, 0.01);
```
python quant_pattern.py --offeset 0.05 --models xxx_xxx.caffemodel
```
3. fine-tune the above quantilized models with fixed pattern (we simply set the learning rate for those convolution layer as 0) prototxt and solver file; 
```
caffe/build/tools/caffe train --solver finetune_solver.txt --weights xxx_xxx_quant_0.05.caffemodel 2>&1 | tee fine_tune.log
```

## Experiments on CIFAR10 with ResNet
In general, different data augumentation techniques results in different test accuracy. In order to obtain better performance, we train ResNet-20,ResNet-32, ResNet-44 and ResNet-55 on cifar10 dataset with three different augumentation techniques. 
1. cifar10 dataset download from [NIN] (https://gist.github.com/ebenolson/91e2cfa51fdb58782c26)
2. cifar10 dataset download from [Highway Network](https://github.com/flukeskywalker/highway-networks)
3. cifar10 dataset download from [Highway Network](https://github.com/flukeskywalker/highway-networks) using additional 4 pixels padded on each side of each image and with GCN preprocessed. 

The following is the comparison Top 1 and Top 5 test accuracy with cifar10 dataset with different augumentation techniques on the four ResNets. 

| Model     | Test Accuracy (1) | Test Accuracy (2) | Test Accuracy (3) |
|-----------|-------------------|-------------------|-------------------|
| ResNet-20 | 0.8204 0.9861     | 0.8893 0.9957     | 0.9118 0.9974     |
| ResNet-32 | 0.8477 0.9899     | 0.9031 0.9963     | 0.9276 0.9972     |
| ResNet-44 | 0.8380 0.9895     | 0.9006 0.9956     | 0.9283 0.9982     |
| ResNet-56 | 0.4555 0.8322     | 0.9027 0.9957     | 0.9375 0.9977     |




## Experiments on ImageNet with GoogleNet
## Experiments on ImageNet with Customized-InceptionNet
## Summary



