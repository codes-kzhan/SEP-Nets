layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
    #mean_file: "/optimus_data/backed_up/cifar10/cifar10-zca-whitened-copy1/mean.binaryproto"
    mean_file: "/Users/zli79/cifar10/cifar10-gcn-leveldb-splits/paddedmean.binaryproto"
  }
  data_param {
    #source: "/optimus_data/backed_up/cifar10/cifar10-zca-whitened-copy1/cifar-train-leveldb/"
    source: "/Users/zli79/cifar10/cifar10-gcn-leveldb-splits/cifar10_full_train_leveldb_padded/"
    #batch_size: 128
    batch_size: 256
    backend: LEVELDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 32
    #mean_file: "/optimus_data/backed_up/cifar10/cifar10-zca-whitened-copy1/mean.binaryproto"
    mean_file: "/Users/zli79/cifar10/cifar10-gcn-leveldb-splits/mean.binaryproto"
  }
  data_param {
    #source: "/optimus_data/backed_up/cifar10/cifar10-zca-whitened-copy1/cifar-test-leveldb/"
    source: "/Users/zli79/cifar10/cifar10-gcn-leveldb-splits/cifar10_test_leveldb_copy/"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "bn_conv1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale_conv1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "map16_1_conv_a"
  type: "Convolution"
  bottom: "conv1"
  top: "map16_1_conv_a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_1_bn_a"
  type: "BatchNorm"
  bottom: "map16_1_conv_a"
  top: "map16_1_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_1_bn_a"
  type: "BatchNorm"
  bottom: "map16_1_conv_a"
  top: "map16_1_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_1_scale_a"
  type: "Scale"
  bottom: "map16_1_conv_a"
  top: "map16_1_conv_a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_1_relu_a"
  type: "ReLU"
  bottom: "map16_1_conv_a"
  top: "map16_1_conv_a"
}
layer {
  name: "map16_1_conv_b"
  type: "Convolution"
  bottom: "map16_1_conv_a"
  top: "map16_1_conv_b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_1_bn_b"
  type: "BatchNorm"
  bottom: "map16_1_conv_b"
  top: "map16_1_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_1_bn_b"
  type: "BatchNorm"
  bottom: "map16_1_conv_b"
  top: "map16_1_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_1_scale_b"
  type: "Scale"
  bottom: "map16_1_conv_b"
  top: "map16_1_conv_b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_1_eltsum"
  type: "Eltwise"
  bottom: "conv1"
  bottom: "map16_1_conv_b"
  top: "map16_1_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_1_relu_after_sum"
  type: "ReLU"
  bottom: "map16_1_eltsum"
  top: "map16_1_eltsum"
}
layer {
  name: "map16_2_conv_a"
  type: "Convolution"
  bottom: "map16_1_eltsum"
  top: "map16_2_conv_a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_2_bn_a"
  type: "BatchNorm"
  bottom: "map16_2_conv_a"
  top: "map16_2_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_2_bn_a"
  type: "BatchNorm"
  bottom: "map16_2_conv_a"
  top: "map16_2_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_2_scale_a"
  type: "Scale"
  bottom: "map16_2_conv_a"
  top: "map16_2_conv_a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_2_relu_a"
  type: "ReLU"
  bottom: "map16_2_conv_a"
  top: "map16_2_conv_a"
}
layer {
  name: "map16_2_conv_b"
  type: "Convolution"
  bottom: "map16_2_conv_a"
  top: "map16_2_conv_b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_2_bn_b"
  type: "BatchNorm"
  bottom: "map16_2_conv_b"
  top: "map16_2_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_2_bn_b"
  type: "BatchNorm"
  bottom: "map16_2_conv_b"
  top: "map16_2_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_2_scale_b"
  type: "Scale"
  bottom: "map16_2_conv_b"
  top: "map16_2_conv_b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_2_eltsum"
  type: "Eltwise"
  bottom: "map16_1_eltsum"
  bottom: "map16_2_conv_b"
  top: "map16_2_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_2_relu_after_sum"
  type: "ReLU"
  bottom: "map16_2_eltsum"
  top: "map16_2_eltsum"
}
layer {
  name: "map16_3_conv_a"
  type: "Convolution"
  bottom: "map16_2_eltsum"
  top: "map16_3_conv_a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_3_bn_a"
  type: "BatchNorm"
  bottom: "map16_3_conv_a"
  top: "map16_3_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_3_bn_a"
  type: "BatchNorm"
  bottom: "map16_3_conv_a"
  top: "map16_3_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_3_scale_a"
  type: "Scale"
  bottom: "map16_3_conv_a"
  top: "map16_3_conv_a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_3_relu_a"
  type: "ReLU"
  bottom: "map16_3_conv_a"
  top: "map16_3_conv_a"
}
layer {
  name: "map16_3_conv_b"
  type: "Convolution"
  bottom: "map16_3_conv_a"
  top: "map16_3_conv_b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_3_bn_b"
  type: "BatchNorm"
  bottom: "map16_3_conv_b"
  top: "map16_3_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_3_bn_b"
  type: "BatchNorm"
  bottom: "map16_3_conv_b"
  top: "map16_3_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_3_scale_b"
  type: "Scale"
  bottom: "map16_3_conv_b"
  top: "map16_3_conv_b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_3_eltsum"
  type: "Eltwise"
  bottom: "map16_2_eltsum"
  bottom: "map16_3_conv_b"
  top: "map16_3_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_3_relu_after_sum"
  type: "ReLU"
  bottom: "map16_3_eltsum"
  top: "map16_3_eltsum"
}
layer {
  name: "map32_1_conv_proj"
  type: "Convolution"
  bottom: "map16_3_eltsum"
  top: "map32_1_conv_proj"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_1_bn_proj"
  type: "BatchNorm"
  bottom: "map32_1_conv_proj"
  top: "map32_1_conv_proj"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_1_bn_proj"
  type: "BatchNorm"
  bottom: "map32_1_conv_proj"
  top: "map32_1_conv_proj"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_1_scale_proj"
  type: "Scale"
  bottom: "map32_1_conv_proj"
  top: "map32_1_conv_proj"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_1_conv_a"
  type: "Convolution"
  bottom: "map16_3_eltsum"
  top: "map32_1_conv_a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_1_bn_a"
  type: "BatchNorm"
  bottom: "map32_1_conv_a"
  top: "map32_1_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_1_bn_a"
  type: "BatchNorm"
  bottom: "map32_1_conv_a"
  top: "map32_1_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_1_scale_a"
  type: "Scale"
  bottom: "map32_1_conv_a"
  top: "map32_1_conv_a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_1_relu_a"
  type: "ReLU"
  bottom: "map32_1_conv_a"
  top: "map32_1_conv_a"
}
layer {
  name: "map32_1_conv_b"
  type: "Convolution"
  bottom: "map32_1_conv_a"
  top: "map32_1_conv_b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_1_bn_b"
  type: "BatchNorm"
  bottom: "map32_1_conv_b"
  top: "map32_1_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_1_bn_b"
  type: "BatchNorm"
  bottom: "map32_1_conv_b"
  top: "map32_1_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_1_scale_b"
  type: "Scale"
  bottom: "map32_1_conv_b"
  top: "map32_1_conv_b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_1_eltsum"
  type: "Eltwise"
  bottom: "map32_1_conv_proj"
  bottom: "map32_1_conv_b"
  top: "map32_1_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_1_relu_after_sum"
  type: "ReLU"
  bottom: "map32_1_eltsum"
  top: "map32_1_eltsum"
}
layer {
  name: "map32_2_conv_a"
  type: "Convolution"
  bottom: "map32_1_eltsum"
  top: "map32_2_conv_a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_2_bn_a"
  type: "BatchNorm"
  bottom: "map32_2_conv_a"
  top: "map32_2_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_2_bn_a"
  type: "BatchNorm"
  bottom: "map32_2_conv_a"
  top: "map32_2_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_2_scale_a"
  type: "Scale"
  bottom: "map32_2_conv_a"
  top: "map32_2_conv_a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_2_relu_a"
  type: "ReLU"
  bottom: "map32_2_conv_a"
  top: "map32_2_conv_a"
}
layer {
  name: "map32_2_conv_b"
  type: "Convolution"
  bottom: "map32_2_conv_a"
  top: "map32_2_conv_b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_2_bn_b"
  type: "BatchNorm"
  bottom: "map32_2_conv_b"
  top: "map32_2_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_2_bn_b"
  type: "BatchNorm"
  bottom: "map32_2_conv_b"
  top: "map32_2_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_2_scale_b"
  type: "Scale"
  bottom: "map32_2_conv_b"
  top: "map32_2_conv_b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_2_eltsum"
  type: "Eltwise"
  bottom: "map32_1_eltsum"
  bottom: "map32_2_conv_b"
  top: "map32_2_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_2_relu_after_sum"
  type: "ReLU"
  bottom: "map32_2_eltsum"
  top: "map32_2_eltsum"
}
layer {
  name: "map32_3_conv_a"
  type: "Convolution"
  bottom: "map32_2_eltsum"
  top: "map32_3_conv_a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_3_bn_a"
  type: "BatchNorm"
  bottom: "map32_3_conv_a"
  top: "map32_3_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_3_bn_a"
  type: "BatchNorm"
  bottom: "map32_3_conv_a"
  top: "map32_3_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_3_scale_a"
  type: "Scale"
  bottom: "map32_3_conv_a"
  top: "map32_3_conv_a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_3_relu_a"
  type: "ReLU"
  bottom: "map32_3_conv_a"
  top: "map32_3_conv_a"
}
layer {
  name: "map32_3_conv_b"
  type: "Convolution"
  bottom: "map32_3_conv_a"
  top: "map32_3_conv_b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_3_bn_b"
  type: "BatchNorm"
  bottom: "map32_3_conv_b"
  top: "map32_3_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_3_bn_b"
  type: "BatchNorm"
  bottom: "map32_3_conv_b"
  top: "map32_3_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_3_scale_b"
  type: "Scale"
  bottom: "map32_3_conv_b"
  top: "map32_3_conv_b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_3_eltsum"
  type: "Eltwise"
  bottom: "map32_2_eltsum"
  bottom: "map32_3_conv_b"
  top: "map32_3_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_3_relu_after_sum"
  type: "ReLU"
  bottom: "map32_3_eltsum"
  top: "map32_3_eltsum"
}
layer {
  name: "map64_1_conv_proj"
  type: "Convolution"
  bottom: "map32_3_eltsum"
  top: "map64_1_conv_proj"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_1_bn_proj"
  type: "BatchNorm"
  bottom: "map64_1_conv_proj"
  top: "map64_1_conv_proj"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_1_bn_proj"
  type: "BatchNorm"
  bottom: "map64_1_conv_proj"
  top: "map64_1_conv_proj"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_1_scale_proj"
  type: "Scale"
  bottom: "map64_1_conv_proj"
  top: "map64_1_conv_proj"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_1_conv_a"
  type: "Convolution"
  bottom: "map32_3_eltsum"
  top: "map64_1_conv_a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_1_bn_a"
  type: "BatchNorm"
  bottom: "map64_1_conv_a"
  top: "map64_1_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_1_bn_a"
  type: "BatchNorm"
  bottom: "map64_1_conv_a"
  top: "map64_1_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_1_scale_a"
  type: "Scale"
  bottom: "map64_1_conv_a"
  top: "map64_1_conv_a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_1_relu_a"
  type: "ReLU"
  bottom: "map64_1_conv_a"
  top: "map64_1_conv_a"
}
layer {
  name: "map64_1_conv_b"
  type: "Convolution"
  bottom: "map64_1_conv_a"
  top: "map64_1_conv_b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_1_bn_b"
  type: "BatchNorm"
  bottom: "map64_1_conv_b"
  top: "map64_1_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_1_bn_b"
  type: "BatchNorm"
  bottom: "map64_1_conv_b"
  top: "map64_1_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_1_scale_b"
  type: "Scale"
  bottom: "map64_1_conv_b"
  top: "map64_1_conv_b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_1_eltsum"
  type: "Eltwise"
  bottom: "map64_1_conv_proj"
  bottom: "map64_1_conv_b"
  top: "map64_1_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_1_relu_after_sum"
  type: "ReLU"
  bottom: "map64_1_eltsum"
  top: "map64_1_eltsum"
}
layer {
  name: "map64_2_conv_a"
  type: "Convolution"
  bottom: "map64_1_eltsum"
  top: "map64_2_conv_a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_2_bn_a"
  type: "BatchNorm"
  bottom: "map64_2_conv_a"
  top: "map64_2_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_2_bn_a"
  type: "BatchNorm"
  bottom: "map64_2_conv_a"
  top: "map64_2_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_2_scale_a"
  type: "Scale"
  bottom: "map64_2_conv_a"
  top: "map64_2_conv_a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_2_relu_a"
  type: "ReLU"
  bottom: "map64_2_conv_a"
  top: "map64_2_conv_a"
}
layer {
  name: "map64_2_conv_b"
  type: "Convolution"
  bottom: "map64_2_conv_a"
  top: "map64_2_conv_b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_2_bn_b"
  type: "BatchNorm"
  bottom: "map64_2_conv_b"
  top: "map64_2_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_2_bn_b"
  type: "BatchNorm"
  bottom: "map64_2_conv_b"
  top: "map64_2_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_2_scale_b"
  type: "Scale"
  bottom: "map64_2_conv_b"
  top: "map64_2_conv_b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_2_eltsum"
  type: "Eltwise"
  bottom: "map64_1_eltsum"
  bottom: "map64_2_conv_b"
  top: "map64_2_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_2_relu_after_sum"
  type: "ReLU"
  bottom: "map64_2_eltsum"
  top: "map64_2_eltsum"
}
layer {
  name: "map64_3_conv_a"
  type: "Convolution"
  bottom: "map64_2_eltsum"
  top: "map64_3_conv_a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_3_bn_a"
  type: "BatchNorm"
  bottom: "map64_3_conv_a"
  top: "map64_3_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_3_bn_a"
  type: "BatchNorm"
  bottom: "map64_3_conv_a"
  top: "map64_3_conv_a"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_3_scale_a"
  type: "Scale"
  bottom: "map64_3_conv_a"
  top: "map64_3_conv_a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_3_relu_a"
  type: "ReLU"
  bottom: "map64_3_conv_a"
  top: "map64_3_conv_a"
}
layer {
  name: "map64_3_conv_b"
  type: "Convolution"
  bottom: "map64_3_conv_a"
  top: "map64_3_conv_b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_3_bn_b"
  type: "BatchNorm"
  bottom: "map64_3_conv_b"
  top: "map64_3_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_3_bn_b"
  type: "BatchNorm"
  bottom: "map64_3_conv_b"
  top: "map64_3_conv_b"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_3_scale_b"
  type: "Scale"
  bottom: "map64_3_conv_b"
  top: "map64_3_conv_b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_3_eltsum"
  type: "Eltwise"
  bottom: "map64_2_eltsum"
  bottom: "map64_3_conv_b"
  top: "map64_3_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_3_relu_after_sum"
  type: "ReLU"
  bottom: "map64_3_eltsum"
  top: "map64_3_eltsum"
}
layer {
  name: "pool_global"
  type: "Pooling"
  bottom: "map64_3_eltsum"
  top: "pool_global"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "score"
  type: "InnerProduct"
  bottom: "pool_global"
  top: "score"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "score"
  bottom: "label"
  top: "loss"
}
layer {
  name: "acc/top1"
  type: "Accuracy"
  bottom: "score"
  bottom: "label"
  top: "acctop1"
  accuracy_param{
	top_k: 1
	}
}

layer {
  name: "acc/top5"
  type: "Accuracy"
  bottom: "score"
  bottom: "label"
  top: "acctop5"
  accuracy_param{
	top_k: 5
	}
}
